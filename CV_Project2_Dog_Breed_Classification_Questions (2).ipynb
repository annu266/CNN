{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kIWaR5ZpKlJ"
   },
   "source": [
    "## Dog Breed Classification\n",
    "\n",
    "In this project we will use traditional CNN, CNN with data augmentation and finally transfer Learning by VGG16 model with weights pre-trained on Imagenet to solve the dog breed classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F7MDmaAw2xGO"
   },
   "source": [
    "### Load Dataset Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZWpQv1OwqYK"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "fVhB9OopxFbX",
    "outputId": "36bba949-7d7c-4b9a-b196-f749a9a07771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "#drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1q2zzIaUprk_"
   },
   "source": [
    "Now, upload the given dataset file shared with you in your google drive and give its path for the below given `project_path` variable. For example, a path is given below according to the file path in our google drive. You need to change this to match the path of yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tp6FvAToxUFs"
   },
   "outputs": [],
   "source": [
    "project_path = \"/content/drive/My Drive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rydR_j8lqUei"
   },
   "source": [
    "Run the below code to extract all the images in the train.zip files given in the dataset. We are going to use these images as train and validation sets and their labels in further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3350WZM4w4EL"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile(project_path+'train.zip', 'r') as z:\n",
    "  z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3NHq1iBCfFjE"
   },
   "source": [
    "Repeat the same step for test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fxzynvB2YCb"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile(project_path+'test.zip', 'r') as z:\n",
    "  z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jnUMhQrDfJmz"
   },
   "source": [
    "Repeat the same step for sample_submission.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PyTxE8q2jLf"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile(project_path+'sample_submission.csv.zip', 'r') as z:\n",
    "  z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2G9RIxB-fOLT"
   },
   "source": [
    "Repeat the same step for labels.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JRHYVnYEkRoY"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile(project_path+'labels.csv.zip', 'r') as z:\n",
    "  z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJc1lVrW_jmL"
   },
   "source": [
    "After this process, we will have 4 files - Train folder, test folder and labels.csv and sample_submission.csv as part of your google drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aYmJKmDqqpng"
   },
   "source": [
    "### Read labels.csv file using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmlJ2VMY96IZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labels=pd.read_csv('labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QP8YAzQvqyK-"
   },
   "source": [
    "### Print the count of each category of Dogs given in the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LXEu-GHa44nQ",
    "outputId": "1cb507c3-c573-4327-ad35-f9dfebd0786d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scottish_deerhound                126\n",
       "maltese_dog                       117\n",
       "afghan_hound                      116\n",
       "entlebucher                       115\n",
       "bernese_mountain_dog              114\n",
       "shih-tzu                          112\n",
       "pomeranian                        111\n",
       "great_pyrenees                    111\n",
       "basenji                           110\n",
       "samoyed                           109\n",
       "airedale                          107\n",
       "tibetan_terrier                   107\n",
       "cairn                             106\n",
       "leonberg                          106\n",
       "beagle                            105\n",
       "japanese_spaniel                  105\n",
       "australian_terrier                102\n",
       "miniature_pinscher                102\n",
       "blenheim_spaniel                  102\n",
       "irish_wolfhound                   101\n",
       "lakeland_terrier                   99\n",
       "saluki                             99\n",
       "papillon                           96\n",
       "whippet                            95\n",
       "norwegian_elkhound                 95\n",
       "siberian_husky                     95\n",
       "pug                                94\n",
       "chow                               93\n",
       "pembroke                           92\n",
       "italian_greyhound                  92\n",
       "                                 ... \n",
       "boxer                              75\n",
       "german_short-haired_pointer        75\n",
       "pekinese                           75\n",
       "bull_mastiff                       75\n",
       "great_dane                         75\n",
       "doberman                           74\n",
       "american_staffordshire_terrier     74\n",
       "cocker_spaniel                     74\n",
       "brittany_spaniel                   73\n",
       "malinois                           73\n",
       "curly-coated_retriever             72\n",
       "redbone                            72\n",
       "flat-coated_retriever              72\n",
       "border_collie                      72\n",
       "standard_schnauzer                 72\n",
       "soft-coated_wheaten_terrier        71\n",
       "kuvasz                             71\n",
       "chihuahua                          71\n",
       "french_bulldog                     70\n",
       "vizsla                             70\n",
       "walker_hound                       69\n",
       "otterhound                         69\n",
       "giant_schnauzer                    69\n",
       "tibetan_mastiff                    69\n",
       "german_shepherd                    69\n",
       "brabancon_griffon                  67\n",
       "komondor                           67\n",
       "golden_retriever                   67\n",
       "eskimo_dog                         66\n",
       "briard                             66\n",
       "Name: breed, Length: 120, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels['breed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhmqGccg3F-f"
   },
   "outputs": [],
   "source": [
    "labels['rank']=labels['breed'].rank(method='dense').astype(int) - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WI94_Qcc0D4M"
   },
   "source": [
    "### Get one-hot encodings of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q48iAcY196I3",
    "outputId": "00ffa333-2d36-4d67-a4ff-91914a1bdc78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alekh\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "#y_train = keras.utils.to_categorical(labels['rank'], 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8o8MQ--HBLTg",
    "outputId": "45770ae1-77a3-4fc3-ddba-3aaf130aec0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10222, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "hEr3ROR3CwrC",
    "outputId": "5ae184e8-2268-4123-8ac6-437d4a447c13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id             breed  rank\n",
       "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull    19\n",
       "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo    37\n",
       "2  001cdf01b096e06d78e9e5112d419397          pekinese    85\n",
       "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick    15\n",
       "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever    49"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWaJ9naXfoiU"
   },
   "source": [
    "## Preparing training dataset\n",
    "1. Write a code which reads each and every id from labels.csv file and loads the corresponding image (in RGB - 128, 128, 3) from the train folder. <br>\n",
    "2. Create 2 variables <br> \n",
    "     a.  x_train - Should have all the images of the dogs from train folder <br>\n",
    "     b.  y_train - Corresponding label of the dog <br>\n",
    "<u>Note:</u> The id of the dog images and its corresponding labels are available in labels.csv file   \n",
    "<u>Hint:</u> Watch the video shared on \"Preparing the training dataset\" if you face issue on creating the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kfzdt4EuB2t-"
   },
   "outputs": [],
   "source": [
    "img_rows=128\n",
    "img_cols=128\n",
    "\n",
    "from tqdm import tqdm\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cSwKM0X46zL_",
    "outputId": "e2731cc2-947f-4e18-b40d-06ac97f93059"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10222/10222 [02:18<00:00, 73.58it/s]\n"
     ]
    }
   ],
   "source": [
    "x_feature = []\n",
    "y_feature = []\n",
    "i = 0 # initialisation\n",
    "for f,img,rk in tqdm(labels.values): # f for format ,jpg\n",
    "    img = cv2.imread('./train/{}.jpg'.format(f), 1)\n",
    "    img_resize = cv2.resize(img, (img_rows, img_cols)) \n",
    "    x_feature.append(img_resize)\n",
    "    y_feature.append(rk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WyYekaJQDiHG"
   },
   "outputs": [],
   "source": [
    "#labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aC2f9ecR0XGR",
    "outputId": "14a700d2-be9d-4b06-be6b-6c07d565d239"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_feature[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nkkZEpOe0ipk"
   },
   "outputs": [],
   "source": [
    "#x_feature[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ioWDEgElBOs"
   },
   "source": [
    "Normalize the training data and convert into 4 dimensions so that it can be used as an input to conv layers in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ARn76j3U1CDa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_feature=np.asarray(x_feature)\n",
    "y_feature=np.asarray(y_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yf6MG3CV5O2u"
   },
   "outputs": [],
   "source": [
    "np.save('x_feature.out', x_feature)\n",
    "np.save('y_feature.out', y_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6VQlD0h1mfXi",
    "outputId": "79207e85-e106-4cb6-e8cb-ae4fb4b41660"
   },
   "outputs": [],
   "source": [
    "#y_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_CSGSpPoTjSn",
    "outputId": "669edb8b-9f2c-4515-dc90-9a1a26ab6c80"
   },
   "outputs": [],
   "source": [
    "#x_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "USK_lxcfURWC",
    "outputId": "781e4165-1dd3-4526-d2a2-2076eb6f55ea"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_feature=np.load('x_feature.out.npy')\n",
    "y_feature=np.load('y_feature.out.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feature=x_feature/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DoK4WqY2VB5H"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alekh\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "y_feature = keras.utils.to_categorical(y_feature, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bdCXuAE11gZL"
   },
   "source": [
    "### Split the training and validation data from `x_train_data` and `y_train_data` obtained from above step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kpWx-pgV96Jv"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "x_train,x_val,y_train,y_val=train_test_split(x_feature,y_feature,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "nkVC5oJwUwcJ",
    "outputId": "dab4ff5e-9c2b-4aeb-a1e6-95a492f5503b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9199, 128, 128, 3)\n",
      "(9199, 120)\n",
      "(1023, 128, 128, 3)\n",
      "(1023, 120)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XkL-N1jDsU8m"
   },
   "source": [
    "### Loading the test data\n",
    "Read the id column from the samples_submission.csv and store it in test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DnpXdpd9b3E7"
   },
   "outputs": [],
   "source": [
    "test_img=pd.read_csv('sample_submission.csv')\n",
    "test_img=test_img['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DEJqZIMbm0Jo"
   },
   "source": [
    "Run the below code to load the test image files in x_test_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zf7n4WG-b3Hv",
    "outputId": "c9e04646-31fe-46bb-ee43-fb8667f361c3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10357/10357 [04:07<00:00, 41.93it/s]\n"
     ]
    }
   ],
   "source": [
    "x_test_feature = []\n",
    "i = 0 # initialisation\n",
    "for f in tqdm(test_img.values): # f for format ,jpg\n",
    "    img = cv2.imread('./test/{}.jpg'.format(f), 1)\n",
    "    img_resize = cv2.resize(img, (img_rows, img_cols)) \n",
    "    x_test_feature.append(img_resize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9My6qSyDnE-_"
   },
   "source": [
    "Normalize the test data and convert it into 4 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "93n-IntMnJGI",
    "outputId": "a8f66463-9f27-4ca0-c37c-3d7833711b52"
   },
   "outputs": [],
   "source": [
    "x_test_feature=np.asarray(x_feature)\n",
    "x_test_feature=x_test_feature/255\n",
    "#x_test_feature = x_test_feature.astype('float32')\n",
    "x_test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_test_feature.out', x_test_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zKezNJVMsocP"
   },
   "source": [
    "### Build a basic conv neural network with 2 conv layers (kernel sizes - 5 and 3) add layers as mentioned below for classification.\n",
    "\n",
    "1. Add a Dense layer with 256 neurons with `relu` activation\n",
    "\n",
    "2. Add a Dense layer with 120 neurons as final layer (as there are 120 classes in the given dataset) with `softmax` activation for classifiaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D2jxTY2S96J4"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "f_BAvCzo96J6",
    "outputId": "cc47cdc8-2588-4bf0-9802-48ab09efac6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 3)\n",
      "Train on 9199 samples, validate on 1023 samples\n",
      "Epoch 1/10\n",
      "9199/9199 [==============================] - 2123s 231ms/step - loss: 4.9186 - acc: 0.0118 - val_loss: 4.7593 - val_acc: 0.0156\n",
      "Epoch 2/10\n",
      "9199/9199 [==============================] - 1706s 185ms/step - loss: 4.3712 - acc: 0.0998 - val_loss: 4.8344 - val_acc: 0.0342\n",
      "Epoch 3/10\n",
      "9199/9199 [==============================] - 1633s 178ms/step - loss: 1.3249 - acc: 0.7075 - val_loss: 8.6750 - val_acc: 0.0274\n",
      "Epoch 4/10\n",
      "9199/9199 [==============================] - 1633s 178ms/step - loss: 0.1410 - acc: 0.9788 - val_loss: 11.4059 - val_acc: 0.0293\n",
      "Epoch 5/10\n",
      "9199/9199 [==============================] - 1641s 178ms/step - loss: 0.0692 - acc: 0.9926 - val_loss: 10.8408 - val_acc: 0.0254\n",
      "Epoch 6/10\n",
      "9199/9199 [==============================] - 1638s 178ms/step - loss: 0.0449 - acc: 0.9948 - val_loss: 11.2080 - val_acc: 0.0196\n",
      "Epoch 7/10\n",
      "9199/9199 [==============================] - 1646s 179ms/step - loss: 0.0335 - acc: 0.9961 - val_loss: 11.0539 - val_acc: 0.0254\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "TRAIN = True\n",
    "img_rows=128\n",
    "img_cols=128\n",
    "\n",
    "if TRAIN:\n",
    "    input_shape = (img_rows,img_cols,3)\n",
    "    print(input_shape)\n",
    "    EPOCHS=10\n",
    "    BATCH_SIZE = 128\n",
    "    \n",
    "    #Initialize the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add a Convolutional Layer with 32 filters of size 3X3 and activation function as 'ReLU' \n",
    "    model.add(Conv2D(32, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape,name='conv_1'))\n",
    "    \n",
    "    #Add a Convolutional Layer with 32 filters of size 3X3 and activation function as 'ReLU' \n",
    "    model.add(Conv2D(32, (3, 3), activation='relu',name='conv_2'))\n",
    "    \n",
    "    #Flatten the layer\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    \n",
    "    #Add Fully Connected Layer with 128 units and activation function as 'ReLU'\n",
    "    model.add(Dense(256, activation='relu',name='dense_1',kernel_initializer='glorot_normal'))\n",
    "    \n",
    "    \n",
    "    #Add Fully Connected Layer with 10 units and activation function as 'softmax'\n",
    "    model.add(Dense(120, activation='softmax',name='dense_2',kernel_initializer='glorot_normal'))\n",
    "    \n",
    "    from keras.optimizers import Adam\n",
    "    from keras.losses import categorical_crossentropy\n",
    "    \n",
    "    #To use adam optimizer for learning weights with learning rate = 0.001\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    #Set the loss function and optimizer for the model training\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Store Training Results\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "    callback_list = [early_stopping]# [stats, early_stopping]\n",
    "        \n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, epochs=EPOCHS,validation_data=(x_val, y_val)\n",
    "              ,callbacks=callback_list \n",
    "              #,verbose=True\n",
    "             )\n",
    "    \n",
    "    model.save('./CV_dog_breed.h5')\n",
    "    \n",
    "else:\n",
    "    print('Loading pretrained model...')\n",
    "    model1 = keras.models.load_model('./CV_dog_breed.h5')\n",
    "    print('Model Loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_and_metrics = model1.evaluate(x_test, y_test)\n",
    "#print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ui8EXw6_oqpR"
   },
   "source": [
    "### Use batch_size = 128 and epochs = 10 and execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab_type": "text",
    "id": "NKqxbqMB0a3r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8hWaKmjoz69"
   },
   "source": [
    "#The model accuracy is very poor !!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "agJKkc6xtKiq"
   },
   "source": [
    "### Use Data Augmentation in the above model to see if the accuracy improves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "31Mn8qnZb3Ru",
    "outputId": "4e138c7b-f658-4af8-bc29-699b08c32bd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  1/287 [..............................] - ETA: 112:34:06 - loss: 10.1421 - acc: 0.0156"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=50,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1  # randomly shift images vertically (fraction of total height)\n",
    "      # randomly flip images\n",
    "    )  # randomly flip images\n",
    "\n",
    "# Prepare the generator\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=128),\n",
    "                    steps_per_epoch=len(x_train) / 32, \n",
    "                    epochs=EPOCHS,callbacks=callback_list )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDLQVFDP96KI"
   },
   "source": [
    "It took lot of time for the above operation thus aborted it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2zmLztqo5DY"
   },
   "source": [
    "# Model accuracy is still poor!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rSTATrhsAo7L"
   },
   "source": [
    "### Lets use Transfer Learning\n",
    "\n",
    "Download the vgg wieght file from here : https://github.com/MinerKasch/applied_deep_learning/blob/master/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zy5JdbW6pIvD"
   },
   "source": [
    "Use the below code to load VGG16 weights trained on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yrqs0zg7ApNw"
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "# Instantiate the model with the pre-trained weights (no top)\n",
    "base_model= VGG16(weights=('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'),\n",
    "                 include_top=False, input_shape = (128, 128, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EItOlRBGpV_A"
   },
   "source": [
    "Print the summary of the base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQsEBgnlpHjH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fHpeOyW0qauW"
   },
   "source": [
    "### Add the following classification layers to the imported VGG Model <br>\n",
    "1. Flatten Layer\n",
    "2. Dense layer with 1024 neurons with activation as Relu\n",
    "3. Dense layer with 256 neurons with activation as Relu\n",
    "4. Dense layer with 120 neurons with activation as Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0BpT4MLkqoaO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alekh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "optimizer = Adam(lr=0.001)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "predictions = Dense(120, activation=\"softmax\")(x)\n",
    "\n",
    "# creating the final model \n",
    "model_final = Model(input = base_model.input, output = predictions)\n",
    "\n",
    "# compile the model \n",
    "model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_feature=np.load('x_feature.out.npy')\n",
    "y_feature=np.load('y_feature.out.npy')\n",
    "\n",
    "x_feature=x_feature/255\n",
    "\n",
    "import keras\n",
    "y_feature = keras.utils.to_categorical(y_feature, 120)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "x_train,x_val,y_train,y_val=train_test_split(x_feature,y_feature,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9199 samples, validate on 1023 samples\n",
      "Epoch 1/20\n",
      "9199/9199 [==============================] - 2647s 288ms/step - loss: 4.7164 - acc: 0.0208 - val_loss: 4.4406 - val_acc: 0.0411\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.04106, saving model to vgg16_best.h5\n",
      "Epoch 2/20\n",
      "9199/9199 [==============================] - 2633s 286ms/step - loss: 4.0089 - acc: 0.0871 - val_loss: 3.8175 - val_acc: 0.0968\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.04106 to 0.09677, saving model to vgg16_best.h5\n",
      "Epoch 3/20\n",
      "9199/9199 [==============================] - 2614s 284ms/step - loss: 3.3448 - acc: 0.1805 - val_loss: 3.5637 - val_acc: 0.1447\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.09677 to 0.14467, saving model to vgg16_best.h5\n",
      "Epoch 4/20\n",
      "9199/9199 [==============================] - 2626s 285ms/step - loss: 2.8649 - acc: 0.2623 - val_loss: 3.3821 - val_acc: 0.1838\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.14467 to 0.18377, saving model to vgg16_best.h5\n",
      "Epoch 5/20\n",
      "9199/9199 [==============================] - 2628s 286ms/step - loss: 2.4779 - acc: 0.3474 - val_loss: 3.3771 - val_acc: 0.1955\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.18377 to 0.19550, saving model to vgg16_best.h5\n",
      "Epoch 6/20\n",
      "9199/9199 [==============================] - 2659s 289ms/step - loss: 2.1325 - acc: 0.4190 - val_loss: 3.4365 - val_acc: 0.1945\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.19550\n",
      "Epoch 7/20\n",
      "9199/9199 [==============================] - 2662s 289ms/step - loss: 1.7589 - acc: 0.5131 - val_loss: 3.4999 - val_acc: 0.2170\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.19550 to 0.21701, saving model to vgg16_best.h5\n",
      "Epoch 8/20\n",
      "9199/9199 [==============================] - 2603s 283ms/step - loss: 1.4330 - acc: 0.5933 - val_loss: 3.7639 - val_acc: 0.2092\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.21701\n",
      "Epoch 9/20\n",
      "9199/9199 [==============================] - 2648s 288ms/step - loss: 1.1252 - acc: 0.6821 - val_loss: 3.9467 - val_acc: 0.2209\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.21701 to 0.22092, saving model to vgg16_best.h5\n",
      "Epoch 10/20\n",
      "9199/9199 [==============================] - 2643s 287ms/step - loss: 0.8399 - acc: 0.7728 - val_loss: 4.3385 - val_acc: 0.2170\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.22092\n",
      "Epoch 11/20\n",
      "9199/9199 [==============================] - 2627s 286ms/step - loss: 0.6031 - acc: 0.8431 - val_loss: 4.5803 - val_acc: 0.2111\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.22092\n",
      "Epoch 12/20\n",
      "9199/9199 [==============================] - 2615s 284ms/step - loss: 0.4368 - acc: 0.8892 - val_loss: 4.9019 - val_acc: 0.2092\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.22092\n",
      "Epoch 13/20\n",
      "9199/9199 [==============================] - 2616s 284ms/step - loss: 0.2892 - acc: 0.9355 - val_loss: 5.2101 - val_acc: 0.2102\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.22092\n",
      "Epoch 14/20\n",
      "9199/9199 [==============================] - 2616s 284ms/step - loss: 0.1874 - acc: 0.9636 - val_loss: 5.5034 - val_acc: 0.2053\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.22092\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19daf168a20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model \n",
    "checkpoint = ModelCheckpoint(\"vgg16_best.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "epochs=20\n",
    "# Train the model \n",
    "model_final.fit(x_train, y_train, epochs = epochs, validation_data=(x_val, y_val), callbacks = [checkpoint, early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.save('./model_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model accuracy increased to 96% but val accuracy is still 20% which is still overfit\n",
    "# Best Val accuracy is 22% at 68% training accuracy again model is overfit"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CV_Project2_Dog_Breed_Classification_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
